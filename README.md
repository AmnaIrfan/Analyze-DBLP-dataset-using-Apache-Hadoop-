# Analyze DBLP dataset using Apache Hadoop Map Reducr

#### Introduction:

#### Instructions:

1. Install [IntelliJ](https://www.jetbrains.com/student/), JDK, Scala runtime, IntelliJ Scala plugin and the [Simple Build Toolkit (SBT)](https://www.scala-sbt.org/1.x/docs/index.html) and make sure that you can run Java monitoring tools.
2. Open the project in IntelliJ and build the project. This may take some time as the Library Dependencies mentioned in the build.sbt file will be downloaded and added to the classpath of the project.
3. Alternatively, you can run the project using command line. Open the terminal and `cd` into the project directory. 

This package includes four different map reduce tasks. Each task is described in detail within its own file.
The configuration files used by the mapreduce sample tasks are located under src/main/resource

All tasks tackle map reduce problems using the DBLP dataset which can be downloaded using the following link
[DBLP XML Dataset](https://dblp.uni-trier.de/xml/)

To get detailed information on how the output for each task was processed and how the full output of each task can be accessed, please read the description.pdf file in the project.

There are three ways to run these map reduce tasks.

#### Run Simulation Locally:
In this course project, you will loop back to your first homework but at a different level. You will solidify the knowledge of resilient overlay networks by designing and implementing a simulator of a cloud computing facility, specifically a reliable overlay network using the Chord algorithm for distribution of work in a cloud datacenter. Your goal is to gain experience with the fundamentals of distributed hash tables (DHTs) and you will experiment with resource provisioning in the cloud environment. You will implement a cloud simulator in Scala using Akka actors and you will build and run your project using the SBT with the runMain command from the command line. In your cloud simulator, you will create the following entities and define interactions among them: actors that simulate users who enter and retrieve data from the cloud, actors who represent servers (i.e., nodes) in the cloud that store the data, and case classes that represent data that are sent to and retrieved from the cloud. The entry point to your simulated cloud will be defined with a RESTful service using [Akka/HTTP](https://doc.akka.io/docs/akka-http/current/introduction.html). 

WARNING: there are a few implementations of cloud simulators and Chord implementations on the Internet. I know about (almost) all of them. You can study these implementations and feel free to use the ideas in your own implementation, and you must acknowledge what you use in your README. However, blindly copying large parts of some existing implementation in your code will result in receiving the grade F for the entire course with the transfer of your case of plagiarism to the Dean of Students Office, which will be followed with severe penalties. Most likely, you will be suspended or complete dismissed from the program in the worst case. Please do not plagiarize existing implementations, it is not worth it!

#### Run Simulation on Docker:
The input to your cloud simulator is the number of users, the number of the computers in the cloud - depending on your RAM and CPU it may be in millions, the minimum and the maximum number of requests per minute that each user actor can send, the duration of the simulation in minutes (more than one and less than 1000), the time marks (e.g., minute 10 during 20min simulation) when the system will capture a global state for testing (see the explanation below), the list of the items in a file (e.g., list of movies that include the title, the year, and the revenue), and the ratio of read/write requests. A read request will retrive an item from the cloud (e.g., a movie using its title/year) and a write request will store an item in the cloud (e.g., uploading a movie using its title/year - of course the GBs of data that contain the actual movie content will not be uploaded). For additional 3% bonus you can integrate your Scala simulator with a [statistical package called R](https://www.r-project.org/) to use its functions to implement the [accept/reject method](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2924739/) for sampling probabilistic distributions, which you can use to model various aspects in your simulator (e.g., the arrival of data items to store and their sizes or the failures of servers). As a result, your simulator is guided by probabilistic distributions with certain predefined parameters (e.g., the mean and the variance for the normal distribution) that you choose.

Thus, you will use a random generator to generate the number of requests for each actor that represents users using some probabilistic distribution of your choice. In fact, you can implement different distributions or select ones from the R package or some other open-source libraries. The semantics of the data (e.g., movies, books, simply records) does not matter - feel free to chose whatever you want. Once created, actors that represent users will generate and send data to the cloud endpoint(s), which will then use the Chord algorithm to deliver this data to the actors that simulate cloud servers to store or to retrieve the data. You will use a logging framework to log all requests sent from actors and received by the cloud and responses that are returned by the cloud actors. The log will serve as the output verification of the functionality of your cloud simulator.

Your main work is in implementing the Chord algorithm using the convergent hashing that we study in class, which is a realization of a DHT protocol that is described in the paper that is the mandatory reading material for this class: Chord: A Scalable Peer-to-peer Lookup Service for Internet Applications. In your simulator, Chord will store key-value pairs and find the value associated with a key that is submitted by an actor, which simulates a user. To accomplish this task, Chord distributes actors that simulate cloud servers over a dynamic network of virtual nodes (you can assume one computer per node), and it implements a protocol for finding these objects once they have been placed in the overlay network. As you can imagine, there is an invisible network that connects cloud servers, which are simulated by the actors, however, these actors impose their own overlay network by using Chord to send messages directly to one another. Every node in this network is simulated as an actor for looking up keys for user actors and for determining which actors will serve as key stores.

### Run Tests
